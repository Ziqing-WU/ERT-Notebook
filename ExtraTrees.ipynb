{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the evaluation of algorithm in machine learning course at ISAE-SUPAERO, written by Ziqing WU.\n",
    "\n",
    "\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Extremely Randomized Trees</div>\n",
    "\n",
    "\n",
    "0. [Preliminary - Brief recall of decision tree and bagging](#sec0) \n",
    "1. [Introduction](#sec1)\n",
    "2. \n",
    "3. [Examples]\n",
    "    1. [Spam or ham?]\n",
    "    2. [NIST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec0\"></a> 0. Preliminary - Brief recall of decision tree and bagging\n",
    "The extremely randomized trees is a tree-based ensemble method of supervised classification and regression problems.\n",
    "\n",
    "To get a thourough understanding of this method, the knowledge of decision tree and bagging that we have covered in class is necessary. In this section, a brief recall of these two concepts is provided. You may skip it if you feel completely comfortable with decision trees and bagging idea. \n",
    "\n",
    "## 0.1 Decision Tree\n",
    "\n",
    "Decision Tree is the hierarchical description of data based on logical (binary) questions. Based on the decision tree, several algorithms are proposed to solve classification or regression problems. They are greedy, top-down, recursive, partitioning. Here CART algorithm for classification is taken as an example. [[Rutkowski, 2014](https://www.sciencedirect.com/science/article/pii/S0020025514000206?casa_token=17o4ZXxeHmsAAAAA:XF-ClnEesKcyOCWN97d2g3RqryCxBm0gOw8qHNKirHzWj7MQQ-PB1EYVyHw70svyuzZ5T9nTXn0#s0010)]\n",
    "\n",
    "+ Algorithm starts with a single node - the root.\n",
    "+ During the learning process, in each created node, the particular subset of training set is processed. If all elements of this set are of the same class, the node will be tagged as a leaf and the split is not made. \n",
    "+ If not, the best attribute to split is chosen according to split measure function.\n",
    "+ The tree can be grown until the node is not split, meaning either the list of available attributes in the node contains only one element or all the elements from the subset are from the same class.\n",
    "\n",
    "The impurity measure used in CART is Gini index. For any subset $S_q$, the fraction of all data elements in cosidered node from class k is denoted by $p_{k,q}$. The Gini index is given by $$Gini(S_q)=1-\\sum_{k=1}^{K}(p_{k,q})^2$$\n",
    "\n",
    "It reaches its minimum (zero) when all cases fall into a single target category, and its maximum when data is equally distributed among all classes. \n",
    "\n",
    "The weighted Gini index of $S_q$ is also introduced by $$wGini(S_q, A)=p_L Gini(L_q(A))+(1-p_L)Gini(R_q(A))$$ where $A$ is the split of $S_q$, $L_q$ and $R_q$ are the two subsets derived from $S_q$ and split $A$, $p_L$ is the fraction of data elements from $S_q$ which belongs to the subset $L_q$. The Gini index of two subsets $L_q$ and $R_q$ are also calculated to deduce the weighted Gini of $S_q$. \n",
    "\n",
    "With Gini index and weighted Gini index, we can define the split measure function, called Gini gain: $$g(S_q)=Gini(S_q)-wGini(S_q, A)$$\n",
    "\n",
    "Then the optimal partition $\\tilde A$ for which Gini gain reaches its maximum is chosen among all possible partitions, showing the greedy property of this algorithm. \n",
    "\n",
    "Other split measure function can be chosen, such as cross-entropy loss. Please refer to the its [Wikipedia page](https://en.wikipedia.org/wiki/Cross_entropy) for more information.\n",
    "\n",
    "The advantages of decision tree are that it is quite easy to explain and interpret, relatively fast with time complexity proportional to the multiplication of number of data points, number of features and the depth of trees. It provides also possibilities to work with categorical variables. However, its disadvantages are quite obvious. Decision tree has high variance, meaning a small variation in data can result in a completely different tree. It can easily lead to overfitting by creating a too complex model. Greedy algorithms cannot guarantee to return the globally optimal decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Bagging\n",
    "\n",
    "As we have seen for the decision trees, its high variance makes it difficult to provide an accurate prediction comparing to other algorithms. In 1990s, ensemble methods were put forward to combine the predictions of several base estimators built with a given learning algorithm. The objective is to improve robustness over a single estimator. \n",
    "\n",
    "Two main families of ensemble methods are identified:\n",
    "+ The averaging methods: Several independent estimators are built and then their estimations are averaged, the variance is reduced comparing to single estimators. These methods work best with strong and complex models, such as fully developed decision trees. \n",
    "+ The boosting methods: base estimators are built sequentially with objective to reduce the bias of the combined estimator. These methods are more adapted to weak models, such as shallow decision trees.\n",
    "\n",
    "In this notebook, we are interested in the averaging methods, with a particular focus on extremely randomized trees. So we may ask what is the principle for the averaging methods? Here a bootstrap aggregating (bagging) idea will be introduced [[Brieman, 1996](https://link-springer-com.rev-doc.isae.fr/article/10.1007/BF00058655)].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more information on these two topics, don't hesitate to check the following links:\n",
    "+ [Decision Tree notebook used in class](https://github.com/erachelson/MLclass/blob/master/8%20-%20Decision%20Trees/Decision%20Trees.ipynb)\n",
    "+ [Bagging notebook used in class](https://github.com/erachelson/MLclass/blob/master/10%20-%20Bagging/Bagging.ipynb)\n",
    "+ [Page scikit learn for decision trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "+ [Page scikit learn for ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a> 1. Introduction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

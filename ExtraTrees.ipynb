{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the evaluation of algorithm in machine learning course at ISAE-SUPAERO, written by Ziqing WU.\n",
    "\n",
    "\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Extremely Randomized Trees</div>\n",
    "\n",
    "\n",
    "0. [Preliminary - Brief recall of decision tree and bagging](#sec0) \n",
    "1. [Introduction](#sec1)\n",
    "2. [Algorithm](#sec2)\n",
    "3. [Examples]\n",
    "    1. [Spam or ham?]\n",
    "    2. [NIST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec0\"></a> 0. Preliminary - Brief recall of decision tree and bagging\n",
    "The extremely randomized trees is a tree-based ensemble method of supervised classification and regression problems.\n",
    "\n",
    "To get a thourough understanding of this method, the knowledge of decision tree and bagging that we have covered in class is necessary. In this section, a brief recall of these two concepts is provided. You may skip it if you feel completely comfortable with decision trees and bagging idea. \n",
    "\n",
    "## 0.1 Decision Tree\n",
    "\n",
    "Decision Tree is the hierarchical description of data based on logical (binary) questions. Based on the decision tree, several algorithms are proposed to solve classification or regression problems. They are greedy, top-down, recursive, partitioning. Here CART (Classification And Regression Trees) algorithm for classification is taken as an example. [[Rutkowski, 2014](https://www.sciencedirect.com/science/article/pii/S0020025514000206?casa_token=17o4ZXxeHmsAAAAA:XF-ClnEesKcyOCWN97d2g3RqryCxBm0gOw8qHNKirHzWj7MQQ-PB1EYVyHw70svyuzZ5T9nTXn0#s0010)]\n",
    "\n",
    "+ Algorithm starts with a single node - the root.\n",
    "+ During the learning process, in each created node, the particular subset of training set is processed. If all elements of this set are of the same class, the node will be tagged as a leaf and the split is not made. \n",
    "+ If not, the best attribute to split is chosen according to split measure function.\n",
    "+ The tree can be grown until the node is not split, meaning either the list of available attributes in the node contains only one element or all the elements from the subset are from the same class.\n",
    "\n",
    "The impurity measure used in CART is Gini index. For any subset $S_q$, the fraction of all data elements in cosidered node from class k is denoted by $p_{k,q}$. The Gini index is given by $$Gini(S_q)=1-\\sum_{k=1}^{K}(p_{k,q})^2$$\n",
    "\n",
    "It reaches its minimum (zero) when all cases fall into a single target category, and its maximum when data is equally distributed among all classes. \n",
    "\n",
    "The weighted Gini index of $S_q$ is also introduced by $$wGini(S_q, A)=p_L Gini(L_q(A))+(1-p_L)Gini(R_q(A))$$ where $A$ is the split of $S_q$, $L_q$ and $R_q$ are the two subsets derived from $S_q$ and split $A$, $p_L$ is the fraction of data elements from $S_q$ which belongs to the subset $L_q$. The Gini index of two subsets $L_q$ and $R_q$ are also calculated to deduce the weighted Gini of $S_q$. \n",
    "\n",
    "With Gini index and weighted Gini index, we can define the split measure function, called Gini gain: $$g(S_q)=Gini(S_q)-wGini(S_q, A)$$\n",
    "\n",
    "Then the optimal partition $\\tilde A$ for which Gini gain reaches its maximum is chosen among all possible partitions, showing the greedy property of this algorithm. \n",
    "\n",
    "Other split measure function can be chosen, such as cross-entropy loss. Please refer to the its [Wikipedia page](https://en.wikipedia.org/wiki/Cross_entropy) for more information.\n",
    "\n",
    "The advantages of decision tree are that it is quite easy to explain and interpret, relatively fast with time complexity proportional to the multiplication of number of data points, number of features and the depth of trees. It provides also possibilities to work with categorical variables. However, its disadvantages are quite obvious. Decision tree has high variance, meaning a small variation in data can result in a completely different tree. It can easily lead to overfitting by creating a too complex model. Greedy algorithms cannot guarantee to return the globally optimal decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Bagging\n",
    "\n",
    "As we have seen for the decision trees, its high variance makes it difficult to provide an accurate prediction comparing to other algorithms. In 1990s, ensemble methods were put forward to combine the predictions of several base estimators built with a given learning algorithm. The objective is to improve robustness over a single estimator. \n",
    "\n",
    "Two main families of ensemble methods are identified:\n",
    "+ The averaging methods: Several independent estimators are built and then their estimations are averaged, the variance is reduced comparing to single estimators. These methods work best with strong and complex models, such as fully developed decision trees. \n",
    "+ The boosting methods: base estimators are built sequentially with objective to reduce the bias of the combined estimator. These methods are more adapted to weak models, such as shallow decision trees.\n",
    "\n",
    "In this notebook, we are interested in the averaging methods, with a particular focus on extremely randomized trees. So we may ask what is the principle for the averaging methods? Here a bootstrap aggregating (bagging) idea will be introduced [[Brieman, 1996](https://link-springer-com.rev-doc.isae.fr/article/10.1007/BF00058655)].\n",
    "\n",
    "A learning set of $\\mathcal{L}$ consists of data $\\{(y_n,\\mathbf{x}_n), n = 1,...,N\\}$ where $y_n$ is class label or a numerical response. Assume that a predictor $\\varphi(x,\\mathcal{L})$ is available to predict $y$ with the input $\\mathbf{x}$. Suppose that we have a sequence of learning sets $\\{\\mathcal{L}_k\\}$ each consisting of $N$ independent observations from the same underlying distribution as $\\mathcal{L}$. The objective is to use the $\\{\\mathcal{L}_k\\}$ to get a better predictor than the single learning set predictor $\\varphi(x,\\mathcal{L})$. The restriction is that we are only allowed to work with the sequence of predictors $\\{\\varphi(\\mathbf{x},\\mathcal{L}_k)\\}$.\n",
    "\n",
    "If $y$ is numerical, the average of $\\varphi(x,\\mathcal{L}_k)$ over $k$ can be used to replace $\\varphi(x,\\mathcal{L})$. If $y$ is a class label, then one method of aggregating the $\\varphi(x,\\mathcal{L}_k)$ is by voting. The class label predicted of a certain input is determined by the number of votes for each class among all predictors $\\{\\varphi(\\mathbf{x},\\mathcal{L}_k)\\}$. The class with most votes is the predicted class for this input.\n",
    "\n",
    "In practice, we usually have a single learning set $\\mathcal{L}$. So we imite the process by taking repeated bootstrap samples from $\\mathcal{L}$ to form a sequence of learning sets. This procedure is called Bagging (\"bootstrap aggregating\"). The learning sets bootstraped approximates the distribution underlying $\\mathcal{L}$. \n",
    "\n",
    "A critical factor in whether bagging will improve accuracy is the stability of the procedure for constructing $\\varphi$. If changes in $\\mathcal{L}$ produces small changes in $\\varphi$, then $\\varphi_B$ will be close to $\\varphi$. Improvement will occur for unstable procedures where a small change in $\\mathcal{L}$ can produce large changes in $\\varphi$. For instance, trees and neural networks can be improved by bagging method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more information on these two topics, don't hesitate to check the following links:\n",
    "+ [Decision Tree notebook used in class](https://github.com/erachelson/MLclass/blob/master/8%20-%20Decision%20Trees/Decision%20Trees.ipynb)\n",
    "+ [Bagging notebook used in class](https://github.com/erachelson/MLclass/blob/master/10%20-%20Bagging/Bagging.ipynb)\n",
    "+ [Page scikit learn for decision trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "+ [Page scikit learn for ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a> 1. Introduction\n",
    "Tree-based methods like CART (Classification And Regression Trees) has high variance. Models induced by these are to a large extent random, and also the splits, both attributes and cut-points that are chosen at each internal node depend on the random nature of learning sample. This high variance is a main reason for the relative poor accuracy obtained by tree-based methods. The \"Bagging\" idea is came up to reduce the variance of a learning algorithm without increasing its bias too much. It introduces randomization into the learning algorithm and exploit at each run a different randomized version of the learning sample, in order to provide an ensemble of diversified models. The predictions of these models are then aggregated by an average for regression problem and a majority vote for classification. \n",
    "\n",
    "Using bagging based on decision trees is an attractive idea because in one way, the accuracy can be improved by bagging comparing to single model, in the other way, the computational cost remains to be low even growing several models is required. In this sense, several techniques for introducing randomness in growing a forest of trees are proposed. We can cite the random subspace method [[Ho, 1998](https://ieeexplore.ieee.org/document/709601)], random forest [[Breiman, 2001](https://link.springer.com/article/10.1023/A:1010933404324)], perfect random tree ensembles [[Cutler, 2001](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.2940&rep=rep1&type=pdf)], etc. These methods cause perturbations in the induced models by modifying the algorithm responsible for the search of the optimal split during tree growing. \n",
    "\n",
    "These methods randomize the standard tree growing algorithm to some extent, but are still far from totally random trees. It is quite interesting to investigate whether higher level of randomization can improve accuracy with respect to the above methods. This is why extremly randomized trees are proposed to select the cut-point fully at random for a given numerical attribute [[Geurts, 2006](https://link.springer.com/article/10.1007/s10994-006-6226-1)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. Algorithm\n",
    "The Extremly Randomized Trees (Extra-Trees) algorithm builds an ensemble of unpruned decision or regression trees according to the top down procedure. The two main characteristics of this algorithm are :\n",
    "+ It splits nodes by choosing cut-points fully at random.\n",
    "+ It uses the whole learning sample to grow the trees.\n",
    "\n",
    "The algorithm is described as below:\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Split_a_node($S$)**\n",
    "    \n",
    "Input: the local learning subset $S$ corresponding to the node we want to split\n",
    "    \n",
    "Output: a split $[a<a_c]$ or nothing\n",
    "\n",
    "+ If **Stop_split($S$)** is TRUE then return nothing.\n",
    "+ Otherwise select K attributes $\\{a_1,...,a_K\\}$ among all non constant (in $S$) candidate attributes;\n",
    "+ Draw K splits $\\{s_1,...,s_K\\}$, where $s_i$ = **Pick_a_random_split**($S$,$a_i$), $\\forall i = 1,...,K$;\n",
    "+ Return a split $s_*$ such that $Score(s_*, S) = max_{i=1,...,K}Score(s_i,S)$\n",
    "    \n",
    "**Pick_a_random_split($S,a$)**    \n",
    "Inputs: a subset $S$ and an attribute $a$\n",
    "    \n",
    "Output: a split\n",
    "\n",
    "+ Let $a_{max}^S$ and $a_{min}^S$ denote the maximal and minimal value of $a$ in $S$;\n",
    "+ Draw a random cut-point $a_c$ uniformly in $[a_{min}^S,a_{max}^S]$;\n",
    "+ Return the split $[a<a_c]$\n",
    "    \n",
    "**Stop_split**\n",
    "\n",
    "Input: a subset $S$\n",
    "Output: a boolean\n",
    "\n",
    "+ If $|S|<n_{min}$, then return TRUE;\n",
    "+ If all attributes are constant in $S$, then return TRUE;\n",
    "+ If the output is constant in $S$, then return TRUE;\n",
    "+ Otherwise, return FALSE.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
